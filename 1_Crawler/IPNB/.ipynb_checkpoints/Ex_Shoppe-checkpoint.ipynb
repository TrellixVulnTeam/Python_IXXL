{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ex19_Shoppe&狀態碼為200但無任何資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "使用偽裝user-agent爬取蝦皮購物網\n",
    "https://freelancerlife.info/zh/blog/python-web-scraping-user-agent-for-shopee/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "實戰中，雖然只要會使用最基本的requests和beautifulsoup的操作就可以爬許多網站，\n",
    "但那只是剛好對方的網站伺服器沒有任何檢查措施，就直接將網站的內容回應給你了。\n",
    "\n",
    "然而只要對方伺服器有任何檢查措施時，你就會得不到你想要的資訊，你可能被拒絕訪問，\n",
    "又或者是被重新導向到一個新的網頁。為了處理這種狀況，在使用requests套件時，\n",
    "可以額外加入user agent，使用虛擬的身份，這樣在拜訪網站時，對方的網頁伺服器如果認可你的身份，\n",
    "你就可以得到正確的網頁內容。\n",
    "\n",
    "以蝦皮購物作為範例，某些網站需要恰當的使用user-agent，才可以成功的獲取網頁內容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 當狀態碼為200，但伺服器卻不回傳給你任何商品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "當你一進入他的首頁時，在搜索欄中輸入你要查找的商品後，你就會跳到商品清單的頁面，請注意這裡的網址。\n",
    "\n",
    "\n",
    "我以PS4 pro 主機，進行搜索，你可以看到網址變成 https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F 。\n",
    "\n",
    "很明顯這是以get的方式傳遞你輸入的變數(keyword)給對方的伺服器後台，之後進資料庫比對將最符合的商品傳出來，呈現給你。\n",
    "\n",
    "因此我們可以試試看手動改一下網址裡的keyword，然後直接用瀏覽器拜訪看看是不是能得到預期的東西。\n",
    "\n",
    "例如(XBOX one 主機): https://shopee.tw/search?keyword=XBOX%20one%20%E4%B8%BB%E6%A9%9F\n",
    "\n",
    "\n",
    "沒問題!你在瀏覽器中可以很簡單的直接改網址來獲得不一樣的商品，因此應該可以很簡單的設計一份網路爬蟲程式來抓取你要的商品。\n",
    "\n",
    "使用我們之前的requests、及beautifulsoup套件，你可以寫出例如以下的程式:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F'\n",
    "r = requests.get(url)\n",
    "print(r.status_code)\n",
    "print(len(r.text))\n",
    "Copy\n",
    "200\n",
    "\n",
    "似乎沒有問題?狀態碼為200，代表沒有問題?\n",
    "\n",
    "接著試著使用beautifulesoup去剖析內容，在瀏覽器中檢查元素，可以得知商品是位在\n",
    "div class=\" col-xs-2-4 shopee-search-item-result__item\"底下，因此可以使用以下程式嘗試去剖析原始碼:\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "items = soup.find_all(\"div\", class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "print(len(items))\n",
    "Copy\n",
    "0\n",
    "\n",
    "沒有任何東西!!\n",
    "\n",
    "是的，你成功地拜訪了網站(status_code == 200)，但是伺服器卻不回傳給你任何商品。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 是不是被重新導向了<Response [302]>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我們可以再檢查看看是不是被重新導向了，被連接到一個新的網址去。\n",
    "\n",
    "可以使用以下指令檢查:\n",
    "\n",
    "print(r.history)\n",
    "print(r.url)\n",
    "Copy\n",
    "[<Response [302]>]\n",
    "https://shopee.tw/search-item/?search=PS4%20pro%20%E4%B8%BB%E6%A9%9F\n",
    "\n",
    "302 可以簡單的理解為該資源原本確實存在，但已經被臨時改變了位置 。ref: https://zh.wikipedia.org/wiki/HTTP_302\n",
    "\n",
    "所以導致無法獲得商品資料。網址沒有改變，但是被臨時改變了位置?\n",
    "\n",
    "為了避免像這樣，status code顯示200，但事實上中途有被重新導向，而誤解為有正確的拜訪網站，\n",
    "我們可以在request.get中加入allow_redirects=False，\n",
    "\n",
    "如下:\n",
    "\n",
    "url = 'https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F'\n",
    "r = requests.get(url,allow_redirects=False)\n",
    "print(r.status_code)\n",
    "Copy\n",
    "302\n",
    "\n",
    "直接出現302，現在我們可以避免中途明明被重新導向過，卻顯示一切正常。\n",
    "但是，我們依然無法獲得商品資訊，該怎麼做呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用user-agent，明確的告訴對方伺服器你是誰，然後如果認同你的身分的話就回傳給你正確的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "這就是這篇教學的重點，使用user-agent，明確的告訴對方伺服器你是誰，然後如果認同你的身分的話就回傳給你正確的結果。\n",
    "\n",
    "我們可以使用fake_useragent隨機產生user-agent，放入requests.get裡面再試一次，\n",
    "\n",
    "請先安裝fake_useragent ref: https://pypi.org/project/fake-useragent/\n",
    "\n",
    "pip install fake-useragent\n",
    "Copy\n",
    "然後，試試看使用隨機的fake_useragent:\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "for i in range(5):\n",
    "    print(ua.random)\n",
    "    print('*-----------------*')\n",
    "Copy\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\n",
    "*-----------------*\n",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.124 Safari/537.36\n",
    "*-----------------*\n",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; ru-RU) AppleWebKit/533.19.4 (KHTML, like Gecko) Version/5.0.3 Safari/533.19.4\n",
    "*-----------------*\n",
    "Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0\n",
    "*-----------------*\n",
    "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\n",
    "*-----------------*\n",
    "\n",
    "OK!\n",
    "\n",
    "接著以以下的方式將useragent放入headers在將放入request.get中，重新執行一次。\n",
    "\n",
    "如果你想當個很有禮貌的爬蟲，可以在額外加入你的email等聯絡方式。\n",
    "\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "url = 'https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F'\n",
    "ua = UserAgent()\n",
    "headers = {\n",
    "    'User-Agent': ua.random,\n",
    "    'From': 'YOUR EMAIL ADDRESS'\n",
    "}\n",
    "\n",
    "r = requests.get(url,headers=headers,allow_redirects=False)\n",
    "print(r.status_code)\n",
    "print(r.history)\n",
    "print(r.url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "items = soup.find_all(\"div\", class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "print(len(items))\n",
    "Copy\n",
    "200\n",
    "[]\n",
    "https://shopee.tw/search-item/?search=PS4%20pro%20%E4%B8%BB%E6%A9%9F\n",
    "0\n",
    "\n",
    "這次成功的進入，並沒有被重新導向，但是依然沒有得到商品資訊。\n",
    "\n",
    "我猜測可能是如果沒有提供user-agent，對方伺服器會將你重新導向，而提供之後，雖然不會重新導向，\n",
    "但是user-agent並非是伺服器所認可的user-agent，就會拒絕將商品的資訊回傳給你，只丟給你一個空空的頁面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接看網站的robots.txt來猜測用哪個user-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "那麼要用哪個user-agent才有可能會成功呢?我們可以直接看他的robots.txt來進行猜測。\n",
    "\n",
    "https://shopee.tw/robots.txt\n",
    "\n",
    "User-Agent:Googlebot\n",
    "User-Agent:Bingbot\n",
    "Crawl-delay:0.1\n",
    "Disallow: /cart/\n",
    "Disallow: /checkout/\n",
    "Disallow: /buyer/\n",
    "Disallow: /user/\n",
    "Disallow: /me/\n",
    "Disallow: /order/\n",
    "Disallow: /daily_discover/\n",
    "Disallow: /mall/just-for-you/\n",
    "Disallow: /mall/*-cat.\n",
    "Disallow: /from_same_shop/\n",
    "Disallow: /you_may_also_like/\n",
    "Disallow: *-i.*/similar?from=flash_sale\n",
    "Disallow: /find_similar_products/\n",
    "Disallow: /top_products\n",
    "Disallow: /search*searchPrefill\n",
    "\n",
    "User-Agent:*\n",
    "Crawl-delay:1\n",
    "Disallow: /cart/\n",
    "Disallow: /checkout/\n",
    "Disallow: /buyer/\n",
    "Disallow: /user/\n",
    "Disallow: /me/\n",
    "Disallow: /order/\n",
    "Disallow: /daily_discover/\n",
    "Disallow: /mall/just-for-you/\n",
    "Disallow: /mall/*-cat.\n",
    "Disallow: /from_same_shop/\n",
    "Disallow: /you_may_also_like/\n",
    "Disallow: *-i.*/similar\n",
    "Disallow: /find_similar_products/\n",
    "Disallow: /top_products\n",
    "Disallow: /search*searchPrefill\n",
    "Copy\n",
    "既然robots.txt中第一個提到的user-agent就是Googlebot，何不以Googlebot來試試呢?\n",
    "\n",
    "url = 'https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F'\n",
    "headers = {\n",
    "    'User-Agent': 'Googlebot',\n",
    "    'From': 'YOUR EMAIL ADDRESS'\n",
    "}\n",
    "\n",
    "r = requests.get(url,headers=headers,allow_redirects=True)\n",
    "print(r.status_code)\n",
    "print(r.history)\n",
    "print(r.url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "items = soup.find_all(\"div\", class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "print(len(items))\n",
    "Copy\n",
    "200\n",
    "[]\n",
    "https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F\n",
    "50\n",
    "\n",
    "成功!! 有50個商品!!\n",
    "\n",
    "使用了Googlebot，對方伺服器認可了你所偽裝的身分，正確的將商品資訊傳給了你。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "這次我們試圖以requests拜訪蝦皮購物網，經過多方嘗試，終於在設定user-agent等於Googlebot的情況下，成功地獲得了商品資訊。\n",
    "筆者也使用過robots.txt裡提到的Bingbot，但是也無法獲得商品資訊，只能說是湊巧成功了。\n",
    "是不是非常驚險刺激呢?爬蟲程式最有趣的地方就在於猜測對方伺服器後台是如何傳送資料，然後設法以各種手段獲得內容。\n",
    "\n",
    "另外，仔細檢查蝦皮購物網的內容後，事實上對方是有api可以快速呼叫商品內容的，\n",
    "如: https://shopee.tw/api/v2/search_items/?by=relevancy&keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F&limit=20&newest=0&order=desc&page_type=search\n",
    "，會得到包含商品資訊的json檔，然而一樣在python底下使用requests試圖爬取時，要用Googlebot才有辦法獲得該資訊。\n",
    "\n",
    "當我們能夠成功地爬取購物網的商品內容後，就有很多可能的應用，例如:\n",
    "\n",
    "(1) 編寫多個不同電商網站的爬蟲程式，放在一起，變成一個比價程式，讓使用者只要輸入商品名稱，\n",
    "就能跳出各大電商網站該商品的價格，讓使用者可以找到哪個購物網有比較便宜的價格。\n",
    "\n",
    "(2) 讓使用者能夠快速的搜尋多項商品並且比較，例如，可以快速地比較PS4和XBox的價格大約為在哪裡。\n",
    "\n",
    "(3) 可以將電商網站的爬蟲程式寫入排程器裡面(如linux底下的crontab)每隔一段時間執行一次，\n",
    "將商品價格儲存下來，如此，經過一段時間，我們就可以進行商品價格波動的分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章節程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def shopee_scraper(keyword,n_page=0,used=False,new=False):\n",
    "    '''\n",
    "    參數說明:\n",
    "        keyword: 商品名稱關鍵字\n",
    "        n_page: 第幾頁(每頁有50個商品)\n",
    "        used: 是否為二手商品?\n",
    "        new: 是否為新商品?\n",
    "    '''\n",
    "    url = f'https://shopee.tw/search?keyword={keyword}&page={n_page}&sortBy=relevancy'\n",
    "    if used:\n",
    "        url += '&newItem=true'\n",
    "    if new:\n",
    "        url += '&usedItem=true'\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Googlebot',\n",
    "        'From': 'YOUR EMAIL ADDRESS'\n",
    "    }\n",
    "    \n",
    "    r = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    contents = soup.find_all(\"div\", class_=\"_1NoI8_ _2gr36I\")\n",
    "    prices = soup.find_all(\"span\", class_=\"_341bF0\")\n",
    "    all_items = soup.find_all(\"div\", class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "    links = [i.find('a').get('href') for i in all_items]\n",
    "    \n",
    "    for c, p, l in zip(contents, prices, links):\n",
    "        print(c.contents[0])\n",
    "        print(p.contents[0])\n",
    "        print('https://shopee.tw/'+l)\n",
    "        print('*---------------------------------*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例:shoppe_Philippines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# 目標URL網址\n",
    "\"\"\"直接在蝦皮搜尋\"\"\"\n",
    "#url = \"https://ph.xiapibuy.com/search?keyword={0}&page={1}\"\n",
    "\"\"\"在電腦配件中搜尋\"\"\"\n",
    "url = \"https://ph.xiapibuy.com/search?category=18596&keyword={0}&subcategory=18613&page={1}\"\n",
    "\"\"\"在電腦遊戲中搜尋\"\"\"\n",
    "#url = \"https://ph.xiapibuy.com/search?category=20718&keyword={0}&subcategory=20729&page={1}\"\n",
    "\n",
    "def get_urls(url, query, start_page, end_page): \n",
    "    urls = []\n",
    "    for page in range(start_page, end_page+1):\n",
    "        urls.append(url.format(query, page))    #query帶入url的{0}、page帶入{1}\n",
    "    return urls\n",
    "\n",
    "def get_resource(url):\n",
    "    ua = UserAgent()\n",
    "    headers = {\"user-agent\": \"Googlebot\"}\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def parse_html(html_str):\n",
    "    return BeautifulSoup(html_str, \"lxml\")\n",
    "\n",
    "def get_goods(soup):\n",
    "    goods = []\n",
    "    rows = soup.find_all(\"div\",class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "\n",
    "    for row in rows:\n",
    "\n",
    "        try:\n",
    "            name = row.find(\"div\", class_=\"_1NoI8_ _16BAGk\").text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        try:\n",
    "            price = row.find(\"span\", class_=\"_341bF0\").text\n",
    "        except:\n",
    "            price = None\n",
    "            \n",
    "        try:\n",
    "            Original_price = row.find(\"div\", class_=\"_1w9jLI QbH7Ig U90Nhh\").text.replace(\"₱\",\"\")\n",
    "        except:\n",
    "            Original_price = None\n",
    "\n",
    "        try:\n",
    "            sold = row.find(\"div\", class_=\"_18SLBt\").text.replace(\" sold\",\"\")\n",
    "            #假如字串有包含K，將K取代掉，並將字串轉為浮點數，再乘以1000\n",
    "            if \"K\" in sold:\n",
    "                sold = sold.replace(\"K\",\"\")\n",
    "                sold = float(sold)*1000\n",
    "        except:\n",
    "            sold = None\n",
    "\n",
    "        try:\n",
    "            link = \"https://ph.xiapibuy.com/\" + row.find(\"a\").get('href')\n",
    "        except:\n",
    "            link = None\n",
    "        \n",
    "        good= [name, price, Original_price, sold, link]\n",
    "        goods.append(good)\n",
    "    return goods\n",
    "\n",
    "def web_scraping_bot(urls):\n",
    "    all_goods = [[\"品名\",\"價格\",\"原價\",\"售出量\", \"網址\"]]  #巢狀清單\n",
    "    page = 1\n",
    "    \n",
    "    for url in urls:\n",
    "        print(\"抓取: 第\" + str(page) + \"頁 網路資料中...\")\n",
    "        page = page + 1\n",
    "        r = get_resource(url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            soup = parse_html(r.text)\n",
    "            goods = get_goods(soup)\n",
    "            all_goods = all_goods + goods\n",
    "            print(\"等待5秒鐘...\")\n",
    "            \n",
    "            #當目前頁數=所有頁數時，跳出迴圈\n",
    "            now_page = soup.find(\"span\", class_=\"shopee-mini-page-controller__current\").text\n",
    "            all_page = soup.find(\"span\", class_=\"shopee-mini-page-controller__total\").text\n",
    "            if now_page == all_page:\n",
    "                break   #已經沒有下一頁            \n",
    "            time.sleep(5) \n",
    "        else:\n",
    "            print(\"HTTP請求錯誤...\")\n",
    "\n",
    "    return all_goods\n",
    "\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf_8_sig\") as fp:  #utf_8_sig:能讓輸出的csv正確顯示中文(utf_8會有亂碼)\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #print(get_resource(URL).history)\n",
    "    print(get_resource(url).status_code)\n",
    "\n",
    "    urls = get_urls(url, \"genius\", 0, 10)\n",
    "    print(urls)\n",
    "\n",
    "    goods = web_scraping_bot(urls)\n",
    "    df = pd.DataFrame(goods)       #用dataframe列出\n",
    "    print(df)\n",
    "    #for good in goods:                #用list列出\n",
    "    #    print(good)\n",
    "    \n",
    "    save_to_csv(goods, \"Shoppe_Philippines_genius.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例:shoppe_Indonisia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# 目標URL網址\n",
    "\"\"\"直接在蝦皮搜尋\"\"\"\n",
    "#url = \"https://id.xiapibuy.com/search?keyword={0}&page={1}\"\n",
    "\"\"\"在電腦配件中搜尋\"\"\"\n",
    "url = \"https://id.xiapibuy.com/search?category=134&keyword={0}&page={1}\"\n",
    "\"\"\"直接搜尋品牌\"\"\"\n",
    "url = \"https://id.xiapibuy.com/search?attrId=14478&attrName=Merek&attrVal={0}&page={1}\"\n",
    "\n",
    "def get_urls(url, query, start_page, end_page): \n",
    "    urls = []\n",
    "    for page in range(start_page, end_page+1):\n",
    "        urls.append(url.format(query, page))    #query帶入url的{0}、page帶入{1}\n",
    "    return urls\n",
    "\n",
    "def get_resource(url):\n",
    "    ua = UserAgent()\n",
    "    headers = {\"user-agent\": \"Googlebot\"}\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def parse_html(html_str):\n",
    "    return BeautifulSoup(html_str, \"lxml\")\n",
    "\n",
    "def get_goods(soup):\n",
    "    goods = []\n",
    "    rows = soup.find_all(\"div\",class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "\n",
    "    for row in rows:\n",
    "\n",
    "        try:\n",
    "            name = row.find(\"div\", class_=\"_1NoI8_ _16BAGk\").text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        try:\n",
    "            price = row.find(\"span\", class_=\"_341bF0\").text\n",
    "        except:\n",
    "            price = None\n",
    "            \n",
    "        try:\n",
    "            Original_price = row.find(\"div\", class_=\"_1w9jLI QbH7Ig U90Nhh\").text.replace(\"Rp\",\"\")\n",
    "        except:\n",
    "            Original_price = None\n",
    "\n",
    "        try:\n",
    "            sold = row.find(\"div\", class_=\"_18SLBt\").text.replace(\" Terjual\",\"\")\n",
    "            #假如字串有包含RB，將RB取代掉，並將字串轉為浮點數，再乘以1000\n",
    "            if \"RB\" in sold:\n",
    "                sold = sold.replace(\"RB\",\"\")\n",
    "                sold = float(sold)*1000\n",
    "        except:\n",
    "            sold = None\n",
    "\n",
    "        try:\n",
    "            link = \"https://id.xiapibuy.com/\" + row.find(\"a\").get('href')\n",
    "        except:\n",
    "            link = None\n",
    "        \n",
    "        good= [name, price, Original_price, sold, link]\n",
    "        goods.append(good)\n",
    "    return goods\n",
    "\n",
    "def web_scraping_bot(urls):\n",
    "    all_goods = [[\"品名\",\"價格(單位:千)\",\"原價(單位:千)\",\"售出量\", \"網址\"]]  #巢狀清單\n",
    "    page = 1\n",
    "    \n",
    "    for url in urls:\n",
    "        print(\"抓取: 第\" + str(page) + \"頁 網路資料中...\")\n",
    "        page = page + 1\n",
    "        r = get_resource(url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            soup = parse_html(r.text)\n",
    "            goods = get_goods(soup)\n",
    "            all_goods = all_goods + goods\n",
    "            print(\"等待5秒鐘...\")\n",
    "            \n",
    "            #當目前頁數=所有頁數時，跳出迴圈\n",
    "            now_page = soup.find(\"span\", class_=\"shopee-mini-page-controller__current\").text\n",
    "            all_page = soup.find(\"span\", class_=\"shopee-mini-page-controller__total\").text\n",
    "            if now_page == all_page:\n",
    "                break   #已經沒有下一頁\n",
    "            time.sleep(5) \n",
    "        else:\n",
    "            print(\"HTTP請求錯誤...\")\n",
    "\n",
    "    return all_goods\n",
    "\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf_8_sig\") as fp:  #utf_8_sig:能讓輸出的csv正確顯示中文(utf_8會有亂碼)\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #print(get_resource(URL).history)\n",
    "    print(get_resource(url).status_code)\n",
    "\n",
    "    urls = get_urls(url, \"genius\", 0, 10)\n",
    "    print(urls)\n",
    "\n",
    "    goods = web_scraping_bot(urls)\n",
    "    df = pd.DataFrame(goods)       #用dataframe列出\n",
    "    print(df)\n",
    "    #for good in goods:                #用list列出\n",
    "    #    print(good)\n",
    "    \n",
    "    save_to_csv(goods, \"Shoppe_Indonisia_ComputerAccessories_genius.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
