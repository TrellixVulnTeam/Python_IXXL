{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT八卦板今日熱門文章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTT web版的html結構算是比較有規則的, 所以也是拿來練爬蟲的好對象, 下面這隻爬蟲的目的是要去找出今日的熱門文章(50推以上), 同時也會去找出今天有哪些5566發文了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's 5566:\n",
      "{'RS5566', 'Bonker5566', 'Rin5566', 'XDDDpupu5566', 'fun5566', 'lianpig5566', 'kameaki5566', 'stock5566', 'nikubou5566', 'Nigger5566', 'scum5566', 'zrct5566', 'Andy5566', 'gangster5566', 'purine5566', 'TomFord5566', 'junkjizz5566', 'laba5566', 'youtu5566', 'dickhole5566', 'fantacy5566', 'ce3255666', 'zyc5566', 'WOWO5566', 'vc5566', 'ARZT5566', 'aass5566', 'PA5566', 'AKB5566'}\n",
      "\n",
      "There are  1002  posts today.\n",
      "Hot post(≥ 50 push): \n",
      "{'title': '[爆卦] 台灣民眾黨-要求陸委會「講清楚說明白」!', 'href': '/bbs/Gossiping/M.1581478722.A.8B2.html', 'push_count': 99, 'author': 'PunkGrass'}\n",
      "{'title': 'Re: [問卦] 有小明之亂的懶人包嗎？', 'href': '/bbs/Gossiping/M.1581476703.A.395.html', 'push_count': 99, 'author': 'terry1020'}\n",
      "{'title': '[新聞] WHO命名COVID-19 我國維持簡稱\"武漢肺炎\"', 'href': '/bbs/Gossiping/M.1581476283.A.98D.html', 'push_count': 99, 'author': 'thouloveme'}\n",
      "{'title': '[新聞] 入場泳客從寶瓶星號下船才1天 花蓮業者', 'href': '/bbs/Gossiping/M.1581475689.A.36C.html', 'push_count': 99, 'author': 'stevenchiang'}\n",
      "{'title': '[新聞] 網路瘋傳仁寶工程師疑似感染武漢肺炎 仁', 'href': '/bbs/Gossiping/M.1581474983.A.8F5.html', 'push_count': 54, 'author': 'smalltwo'}\n",
      "{'title': '[新聞] 「你先領」卻自己戴口罩遭批 林楚茵：防', 'href': '/bbs/Gossiping/M.1581475498.A.D64.html', 'push_count': 99, 'author': 'SAgirl'}\n",
      "{'title': '[新聞] 掃光京都口罩！陸女大賺500萬災難財', 'href': '/bbs/Gossiping/M.1581474178.A.B0E.html', 'push_count': 85, 'author': 'SuperSg'}\n",
      "{'title': '[新聞] 汐止機車追撞左轉賓士車 騎士為超車送醫', 'href': '/bbs/Gossiping/M.1581473427.A.616.html', 'push_count': 64, 'author': 'Aqqqa'}\n",
      "{'title': '[新聞] 出現新冠肺炎病徵 鍾南山呼籲：不要在家隔離', 'href': '/bbs/Gossiping/M.1581472750.A.A47.html', 'push_count': 99, 'author': 'currykukuo'}\n",
      "{'title': '[爆卦] 時力出聲了-呼籲陸委會', 'href': '/bbs/Gossiping/M.1581471316.A.FBB.html', 'push_count': 99, 'author': 'octopus4406'}\n",
      "{'title': '[爆卦] 為搶消毒水17歲女持刀殺傷7旬婦9歲幼女', 'href': '/bbs/Gossiping/M.1581471601.A.B42.html', 'push_count': 56, 'author': 'nomorepipe'}\n",
      "{'title': '[爆卦] DPP 2020紀念飛行夾克 開賣啦！', 'href': '/bbs/Gossiping/M.1581469691.A.E2B.html', 'push_count': 91, 'author': 'currykukuo'}\n",
      "{'title': '[問卦] 台灣人有可能跟中國大陸人做朋友嗎？', 'href': '/bbs/Gossiping/M.1581468257.A.1BE.html', 'push_count': 75, 'author': 'parttime'}\n",
      "{'title': '[新聞] 快訊／「鑽石公主號」再39例確診！累積17', 'href': '/bbs/Gossiping/M.1581468365.A.AEC.html', 'push_count': 81, 'author': 'osuki'}\n",
      "{'title': '[新聞] 習近平警告防疫太過損害經濟 國務院下令', 'href': '/bbs/Gossiping/M.1581467581.A.B21.html', 'push_count': 98, 'author': 'z770808'}\n",
      "{'title': '[爆卦] 周玉蔻說小明一共有千餘人', 'href': '/bbs/Gossiping/M.1581468044.A.EBA.html', 'push_count': 99, 'author': 'IronCube'}\n",
      "{'title': '[爆卦] 鑽石公主號新增39例', 'href': '/bbs/Gossiping/M.1581467016.A.C40.html', 'push_count': 60, 'author': 'BURNFISH'}\n",
      "{'title': '[問卦] FB的817好像都無聲無息了？', 'href': '/bbs/Gossiping/M.1581467400.A.F6A.html', 'push_count': 69, 'author': 'JeremyYi'}\n",
      "{'title': '[問卦] 怎麼讓政府知道「他小明 我不ok」?', 'href': '/bbs/Gossiping/M.1581466286.A.04B.html', 'push_count': 54, 'author': 'FD56'}\n",
      "{'title': '[爆卦] 川普提案將美國對WHO的金援砍半', 'href': '/bbs/Gossiping/M.1581465006.A.C07.html', 'push_count': 75, 'author': 'Scion'}\n",
      "{'title': '[爆卦] 鑽石公主號再加39人', 'href': '/bbs/Gossiping/M.1581465118.A.27F.html', 'push_count': 99, 'author': 'xxLolaxx'}\n",
      "{'title': '[問卦] qn什麼時候變成政治評論員了?', 'href': '/bbs/Gossiping/M.1581462984.A.F2F.html', 'push_count': 99, 'author': 'SCEW'}\n",
      "{'title': '[新聞] 馬拉威法院宣布民進黨總統當選無效', 'href': '/bbs/Gossiping/M.1581463331.A.883.html', 'push_count': 67, 'author': 'a10141013'}\n",
      "{'title': '[爆卦] 44404確診 1112死亡', 'href': '/bbs/Gossiping/M.1581463984.A.288.html', 'push_count': 81, 'author': 'eddisontw'}\n",
      "{'title': '[新聞] 豬價跌破10年來低價大關 市場：供過於求', 'href': '/bbs/Gossiping/M.1581464391.A.508.html', 'push_count': 92, 'author': 'loserfatotak'}\n",
      "{'title': '[問卦] 真的會有父母把小孩丟在中國?', 'href': '/bbs/Gossiping/M.1581461147.A.83D.html', 'push_count': 52, 'author': 'jeffery95099'}\n",
      "{'title': '[問卦] 約克羊畫的圖好可愛', 'href': '/bbs/Gossiping/M.1581452709.A.003.html', 'push_count': 62, 'author': 'david0426'}\n",
      "{'title': 'Re: [新聞] 陸配子女返台政策一日三變 陸委會再', 'href': '/bbs/Gossiping/M.1581453126.A.030.html', 'push_count': 99, 'author': 'sabinetw'}\n",
      "{'title': 'Re: [問卦] 第一波包機名單政府真的不知情？', 'href': '/bbs/Gossiping/M.1581455456.A.872.html', 'push_count': 59, 'author': 'aeolus811tw'}\n",
      "{'title': 'Re: [問卦] 陸委會事件，我有些話想說', 'href': '/bbs/Gossiping/M.1581451951.A.534.html', 'push_count': 56, 'author': 'Longchamp'}\n",
      "{'title': '[問卦] 為什麼港仔要這麼討人厭?', 'href': '/bbs/Gossiping/M.1581448597.A.9DB.html', 'push_count': 98, 'author': 'XSR700'}\n",
      "{'title': '[問卦] 必須為陸委會說些話', 'href': '/bbs/Gossiping/M.1581447291.A.94F.html', 'push_count': 99, 'author': 'pk698326889'}\n",
      "{'title': '[問卦] 會不會開放小明一天然後又轉彎', 'href': '/bbs/Gossiping/M.1581448025.A.047.html', 'push_count': 99, 'author': 'sulabird'}\n",
      "{'title': 'Re: [問卦] 陸委會一事  國民黨怎沒人出來罵?', 'href': '/bbs/Gossiping/M.1581444048.A.04C.html', 'push_count': 99, 'author': 'panzer1224'}\n",
      "{'title': 'Fw: [新聞] 陸委會新聞稿-024-陸配子女入境管制政策之說明', 'href': '/bbs/Gossiping/M.1581442913.A.13F.html', 'push_count': 84, 'author': 'fragmentwing'}\n",
      "{'title': '[問卦] 小明強制隔離14天可以接受嗎?', 'href': '/bbs/Gossiping/M.1581440393.A.1CB.html', 'push_count': 56, 'author': 'marijnkops'}\n",
      "{'title': '[新聞] 曾放話在機師餐加料！郭芷嫣遭長榮解雇', 'href': '/bbs/Gossiping/M.1581439395.A.30E.html', 'push_count': 99, 'author': 'karen8097'}\n",
      "{'title': '[問卦] COVID-19怎唸', 'href': '/bbs/Gossiping/M.1581439484.A.332.html', 'push_count': 68, 'author': 'dododada'}\n",
      "{'title': '[爆卦] 英國最新兩例可能在監獄裡面發現！', 'href': '/bbs/Gossiping/M.1581438343.A.494.html', 'push_count': 93, 'author': 'jimmylily'}\n",
      "{'title': 'Re: [新聞] 陸配子女返台政策一日三變 陸委會再限制', 'href': '/bbs/Gossiping/M.1581438463.A.E82.html', 'push_count': 99, 'author': 'zball'}\n",
      "{'title': '[問卦] 今晚很多關鍵字的出現頻率異常？？', 'href': '/bbs/Gossiping/M.1581438139.A.5CE.html', 'push_count': 54, 'author': 'offstage'}\n",
      "{'title': 'Re: [新聞] 陸配子女返台政策一日三變 陸委會再限制', 'href': '/bbs/Gossiping/M.1581438211.A.9E0.html', 'push_count': 99, 'author': 'dodobaho'}\n",
      "{'title': '[問卦] 有沒有身邊覺青都消失了的八卦', 'href': '/bbs/Gossiping/M.1581437517.A.122.html', 'push_count': 99, 'author': 'IDLONG'}\n",
      "{'title': '[爆卦] 整點啦 小明可以準備了', 'href': '/bbs/Gossiping/M.1581436841.A.8F0.html', 'push_count': 87, 'author': 'paralux'}\n",
      "{'title': '[問卦] 第一波包機名單政府真的不知情？', 'href': '/bbs/Gossiping/M.1581437036.A.08D.html', 'push_count': 92, 'author': 'reppoc'}\n",
      "------------------------\n",
      "[] /bbs/Gossiping/index39175.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc'\n",
    "\n",
    "\n",
    "def get_web_page(url):\n",
    "    resp = requests.get(url=url, cookies={'over18': '1'})\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url:', resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "\n",
    "\n",
    "def get_articles(dom, date):\n",
    "    soup = BeautifulSoup(dom, 'html5lib')  #\"dom\"是main()爬蟲主程式中的輸入值\n",
    "    # 取得該頁面上一頁連結\n",
    "    paging_div = soup.find('div', 'btn-group btn-group-paging')  #換頁區塊標籤位置\n",
    "    prev_url = paging_div.find_all('a')[1]['href']   #<a>的第2個'href'為'上一頁'的連結\n",
    "\n",
    "    articles = []  # 用來儲存文章資料\n",
    "    divs = soup.find_all('div', 'r-ent')  #文章區塊標籤\n",
    "    for d in divs:\n",
    "        # If post date matched:\n",
    "        if d.find('div', 'date').text.strip() == date:  #\"date\"是main()爬蟲主程式中的輸入值\n",
    "            # 取得推文數\n",
    "            push_count = 0\n",
    "            push_str = d.find('div', 'nrec').text\n",
    "            if push_str:\n",
    "                try:\n",
    "                    push_count = int(push_str)\n",
    "                except ValueError:\n",
    "                    # If transform failed, it might be '爆', 'X1', 'X2', etc.\n",
    "                    if push_str == '爆':\n",
    "                        push_count = 99\n",
    "                    elif push_str.startswith('X'):  #startswith() :檢查字符串是否是以指定子字符串開頭，如果是則返回True否則False\n",
    "                        push_count = -10\n",
    "\n",
    "            # To retrieve title and href of the article:\n",
    "            if d.find('a'):\n",
    "                href = d.find('a')['href']\n",
    "                title = d.find('a').text\n",
    "                author = d.find('div', 'author').text if d.find('div', 'author') else ''  #後面看不懂\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'href': href,\n",
    "                    'push_count': push_count,\n",
    "                    'author': author\n",
    "                })\n",
    "\n",
    "    return articles, prev_url  # 顯示為: [] /bbs/Gossiping/index39175.html\n",
    "\n",
    "\n",
    "def get_author_ids(posts, pattern):  #只取作者id資訊\n",
    "    ids = set()  #set() 函數創建一個無序不重複元素集，可進行關係測試，刪除重複數據，還可以計算交集、差集、並集等\n",
    "    for post in posts:  #main()中的posts=articles，也就是前面的文章資料\n",
    "        if pattern in post['author']:  #假如pattern再articles資料的['author']中\n",
    "            ids.add(post['author'])   #就加入ids這個集合中\n",
    "    return ids\n",
    "\n",
    "\n",
    "def main():  #爬蟲主程式\n",
    "    current_page = get_web_page(PTT_URL + '/bbs/Gossiping/index.html')\n",
    "    if current_page:\n",
    "        # To keep all of today's articles.\n",
    "        articles = [] \n",
    "        today = time.strftime(\"%m/%d\").lstrip('0')  #今天的日期，在這裡我們刪除開頭的0以匹配PTT日期的格式\n",
    "                                    # strftime() : http://tw.gitbook.net/python/time_strftime.html\n",
    "                                    # lstrip() : http://tw.gitbook.net/python/string_lstrip.html\n",
    "#這範圍不太懂============================================================================================================\n",
    "        current_articles, prev_url = get_articles(current_page, today)\n",
    "\n",
    "        while current_articles:\n",
    "            articles += current_articles  #更新值 (就地加) ，每執行一次，articles = []就新增current_articles\n",
    "            current_page = get_web_page(PTT_URL + prev_url)\n",
    "            current_articles, prev_url = get_articles(current_page, today)\n",
    "#這範圍不太懂============================================================================================================\n",
    "        print(\"Today's 5566:\")\n",
    "        print(get_author_ids(articles, '5566'))\n",
    "\n",
    "        print('\\nThere are ', len(articles), ' posts today.')\n",
    "        threshold = 50   #threshold 門檻訂為50\n",
    "        print('Hot post(≥ %d push): ' % threshold)  #正則表達式-顯示為: \"Hot post(≥ 50 push):\" \n",
    "        for article in articles:\n",
    "            if int(article['push_count']) > threshold:  #列出推數大於門檻的文章\n",
    "                print(article)\n",
    "                \n",
    "        # with as: https://openhome.cc/Gossip/Python/WithAs.html\n",
    "        # json.dump: http://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p02_read-write_json_data.html\n",
    "        with open('gossiping.json', 'w', encoding='UTF-8') as file:\n",
    "            json.dump(articles, file, indent=2, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2c6597472504>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "a=2\n",
    "b=\"b\"\n",
    "a,b = 3, 9,8\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT表特版下載器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beauty from: {'title': '[正妹] hsiai.tw', 'href': '/bbs/Beauty/M.1581574657.A.0F4.html', 'push_count': 4, 'author': 'Arschloch'}\n",
      "Collecting beauty from: {'title': '[正妹] Florence Pugh ', 'href': '/bbs/Beauty/M.1581577440.A.B06.html', 'push_count': 2, 'author': 'lateao'}\n",
      "Collecting beauty from: {'title': '[正妹] 童貞を殺すセーター', 'href': '/bbs/Beauty/M.1581580664.A.AC2.html', 'push_count': 1, 'author': 'ring1002'}\n",
      "Collecting beauty from: {'title': '[正妹] 王怡仁', 'href': '/bbs/Beauty/M.1581584581.A.65B.html', 'push_count': 10, 'author': 'gto3ping'}\n",
      "Collecting beauty from: {'title': '[神人] 有女友feel 生氣也很可愛', 'href': '/bbs/Beauty/M.1581584658.A.2BA.html', 'push_count': 0, 'author': 'e8315402'}\n",
      "Collecting beauty from: {'title': '[正妹] 木村文乃三枚', 'href': '/bbs/Beauty/M.1581523986.A.6C8.html', 'push_count': 18, 'author': 'LeonRai'}\n",
      "Collecting beauty from: {'title': '[正妹] Kawai Asuna', 'href': '/bbs/Beauty/M.1581554731.A.99A.html', 'push_count': 14, 'author': 'ring1002'}\n",
      "Collecting beauty from: {'title': '[神人] NHK早晨新聞', 'href': '/bbs/Beauty/M.1581558894.A.0E6.html', 'push_count': 33, 'author': 'airgp32002'}\n",
      "Collecting beauty from: {'title': '[正妹] 大大 吉利', 'href': '/bbs/Beauty/M.1581564147.A.17B.html', 'push_count': 8, 'author': 'riyan'}\n",
      "Collecting beauty from: {'title': '[廣告] 又巨又軟 東坂みゆ', 'href': '/bbs/Beauty/M.1581565207.A.054.html', 'push_count': 10, 'author': 'graperson'}\n",
      "Collecting beauty from: {'title': '[正妹] 本仮屋ユイカ', 'href': '/bbs/Beauty/M.1581572262.A.D3F.html', 'push_count': 2, 'author': 'ring1002'}\n",
      "Collecting beauty from: {'title': '[正妹] 青筋', 'href': '/bbs/Beauty/M.1581573075.A.BB8.html', 'push_count': 42, 'author': 'epili'}\n",
      "Collecting beauty from: {'title': '[正妹] 喜歡戶外活動的朋友 ', 'href': '/bbs/Beauty/M.1581573162.A.463.html', 'push_count': 26, 'author': 'teramars'}\n",
      "Collecting beauty from: {'title': '[正妹] 寺本莉緒', 'href': '/bbs/Beauty/M.1581573753.A.C66.html', 'push_count': 11, 'author': 'ring1002'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc'\n",
    "\n",
    "\n",
    "def get_web_content(url):\n",
    "    resp = requests.get(url=url, cookies={'over18': '1'})\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url: ' + resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "\n",
    "\n",
    "def get_articles(dom, date):\n",
    "    soup = BeautifulSoup(dom, 'html5lib')\n",
    "\n",
    "    paging_dev = soup.find('div', 'btn-group btn-group-paging')\n",
    "    prev_url = paging_dev.find_all('a')[1]['href']\n",
    "\n",
    "    articles = []\n",
    "    divs = soup.find_all('div', 'r-ent')\n",
    "    for div in divs:\n",
    "        if div.find('div', 'date').text.strip() == date:\n",
    "            push_count = 0\n",
    "            push_str = div.find('div', 'nrec').text\n",
    "            if push_str:\n",
    "                try:\n",
    "                    push_count = int(push_str)\n",
    "                except ValueError:\n",
    "                    if push_str == '爆':\n",
    "                        push_count = 99\n",
    "                    elif push_str.startswith('X'):\n",
    "                        push_count = -10\n",
    "\n",
    "            if div.find('a'):\n",
    "                href = div.find('a')['href']\n",
    "                title = div.find('a').text\n",
    "                author = div.find('div', 'author').text if div.find('div', 'author') else ''\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'href': href,\n",
    "                    'push_count': push_count,\n",
    "                    'author': author\n",
    "                })\n",
    "    return articles, prev_url\n",
    "\n",
    "\n",
    "def parse(dom):\n",
    "    soup = BeautifulSoup(dom, 'html.parser')\n",
    "    links = soup.find(id='main-content').find_all('a')\n",
    "    img_urls = []\n",
    "    for link in links:\n",
    "        if re.match(r'^https?://(i.)?(m.)?imgur.com', link['href']):\n",
    "            img_urls.append(link['href'])\n",
    "    return img_urls\n",
    "\n",
    "\n",
    "def save(img_urls, title):\n",
    "    if img_urls:\n",
    "        try:\n",
    "            folder_name = title.strip()\n",
    "            os.makedirs(folder_name)\n",
    "            for img_url in img_urls:\n",
    "                # e.g. 'http://imgur.com/9487qqq.jpg'.split('//') -> ['http:', 'imgur.com/9487qqq.jpg']\n",
    "                if img_url.split('//')[1].startswith('m.'):\n",
    "                    img_url = img_url.replace('//m.', '//i.')\n",
    "                if not img_url.split('//')[1].startswith('i.'):\n",
    "                    img_url = img_url.split('//')[0] + '//i.' + img_url.split('//')[1]\n",
    "                if not img_url.endswith('.jpg'):\n",
    "                    img_url += '.jpg'\n",
    "                file_name = img_url.split('/')[-1]\n",
    "                urllib.request.urlretrieve(img_url, os.path.join(folder_name, file_name))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def main():\n",
    "    current_page = get_web_content(PTT_URL + '/bbs/Beauty/index.html')\n",
    "    if current_page:\n",
    "        articles = []\n",
    "        date = time.strftime(\"%m/%d\").lstrip('0')\n",
    "        current_articles, prev_url = get_articles(current_page, date)\n",
    "        while current_articles:\n",
    "            articles += current_articles\n",
    "            current_page = get_web_content(PTT_URL + prev_url)\n",
    "            current_articles, prev_url = get_articles(current_page, date)\n",
    "\n",
    "        for article in articles:\n",
    "            print('Collecting beauty from:', article)\n",
    "            page = get_web_content(PTT_URL + article['href'])\n",
    "            if page:\n",
    "                img_urls = parse(page)\n",
    "                save(img_urls, article['title'])\n",
    "                article['num_image'] = len(img_urls)\n",
    "\n",
    "        with open('data.json', 'w', encoding='utf-8') as file:\n",
    "            json.dump(articles, file, indent=2, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
