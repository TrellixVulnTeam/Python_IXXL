{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "之前的章節有提到過開發爬蟲的手段, 如果有人已經提供你API了, 那當然就不用去爬網頁了, 接下來的幾隻爬蟲就是基於API來開發的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 八卦版鄉民從哪來?\n",
    "這隻爬蟲會去爬當前八卦版前50篇文章, 然後看這些發文的鄉民是來自哪個國家:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得今日文章列表:\n",
      "共 1077 篇文章\n",
      "取得前50篇文章的IP:\n",
      "查詢 IP: Re: [問卦] 我們的言論自由當初是誰幫忙爭取的？\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d17457537bf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-d17457537bf2>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[0mip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[0mcountry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_country\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcountry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcountry_to_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[0mcountry_to_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcountry\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d17457537bf2>\u001b[0m in \u001b[0;36mget_country\u001b[1;34m(ip)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_country\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFREEGEOIP_API\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mcountry_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'country_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'country_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcountry_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc'\n",
    "FREEGEOIP_API = 'http://freegeoip.net/json/'\n",
    "\n",
    "\n",
    "def get_web_page(url):\n",
    "    resp = requests.get(url=url, cookies={'over18': '1'})\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url: ', resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "\n",
    "\n",
    "def get_articles(dom, date):\n",
    "    soup = BeautifulSoup(dom, 'html5lib')\n",
    "    # Retrieve the link of previous page\n",
    "    paging_div = soup.find('div', 'btn-group btn-group-paging')\n",
    "    prev_url = paging_div.find_all('a')[1]['href']\n",
    "\n",
    "    articles = []\n",
    "    divs = soup.find_all('div', 'r-ent')\n",
    "    for d in divs:\n",
    "        # If post date matched:\n",
    "        if d.find('div', 'date').text.strip() == date:\n",
    "            # To retrieve the push count:\n",
    "            push_count = 0\n",
    "            push_str = d.find('div', 'nrec').text\n",
    "            if push_str:\n",
    "                try:\n",
    "                    push_count = int(push_str)\n",
    "                except ValueError:\n",
    "                    # If transform failed, it might be '爆', 'X1', 'X2', etc.\n",
    "                    if push_str == '爆':\n",
    "                        push_count = 99\n",
    "                    elif push_str.startswith('X'):\n",
    "                        push_count = -10\n",
    "\n",
    "            # To retrieve title and href of the article:\n",
    "            if d.find('a'):\n",
    "                href = d.find('a')['href']\n",
    "                title = d.find('a').text\n",
    "                author = d.find('div', 'author').text if d.find('div', 'author') else ''\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'href': href,\n",
    "                    'push_count': push_count,\n",
    "                    'author': author\n",
    "                })\n",
    "\n",
    "    return articles, prev_url\n",
    "\n",
    "\n",
    "def get_ip(dom):\n",
    "    # e.g., ※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 27.52.6.175\n",
    "    pattern = '來自: \\d+\\.\\d+\\.\\d+\\.\\d+'\n",
    "    match = re.search(pattern, dom)\n",
    "    if match:\n",
    "        return match.group(0).replace('來自: ', '')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_country(ip):\n",
    "    if ip:\n",
    "        data = json.loads(requests.get(FREEGEOIP_API + ip).text)\n",
    "        country_name = data['country_name'] if data['country_name'] else None\n",
    "        return country_name\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('取得今日文章列表:')\n",
    "    current_page = get_web_page(PTT_URL + '/bbs/Gossiping/index.html')\n",
    "    if current_page:\n",
    "        articles = []\n",
    "        today = time.strftime('%m/%d').lstrip('0')\n",
    "        current_articles, prev_url = get_articles(current_page, today)\n",
    "        while current_articles:\n",
    "            articles += current_articles\n",
    "            current_page = get_web_page(PTT_URL + prev_url)\n",
    "            current_articles, prev_url = get_articles(current_page, today)\n",
    "        print('共 %d 篇文章' % (len(articles)))\n",
    "\n",
    "        print('取得前50篇文章的IP:')\n",
    "        country_to_count = dict()\n",
    "        for article in articles[:50]:\n",
    "            print('查詢 IP:', article['title'])\n",
    "            page = get_web_page(PTT_URL + article['href'])\n",
    "            if page:\n",
    "                ip = get_ip(page)\n",
    "                country = get_country(ip)\n",
    "                if country in country_to_count.keys():\n",
    "                    country_to_count[country] += 1\n",
    "                else:\n",
    "                    country_to_count[country] = 1\n",
    "\n",
    "        print('各國IP分佈: ')\n",
    "        for k, v in country_to_count.items():\n",
    "            print(k, v)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook Graph API\n",
    "* 要使用FB Graph API, 要先取得自己的token, 可以到這個網站去申請: https://developers.facebook.com/tools/explorer\n",
    "* 點選\"取得token\" -> \"取得用戶存取token\" -> 勾選你想讓這個token可以取得的資訊 -> 得到token\n",
    "* 把這個token記著, 等等程式裡面要用(ACCESS_TOKEN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "# To obtain the access token, go to https://developers.facebook.com/tools/explorer.\n",
    "ACCESS_TOKEN = ''\n",
    "\n",
    "\n",
    "def get_my_friends():\n",
    "    url = 'https://graph.facebook.com/v2.9/me?fields=id,name,friends&access_token={}'.format(ACCESS_TOKEN)\n",
    "    data = requests.get(url).json()\n",
    "    print('My ID: ' + data['id'])\n",
    "    print('My name: ' + data['name'])\n",
    "    print('Total friends: ', data['friends']['summary']['total_count'], 'friends.')\n",
    "\n",
    "\n",
    "def get_page_post(page_id):\n",
    "    url = 'https://graph.facebook.com/v2.9/{0}/posts?access_token={1}'.format(page_id, ACCESS_TOKEN)\n",
    "    data = requests.get(url).json()\n",
    "    print('There are ', len(data['data']), ' posts on the fans page.')\n",
    "    print('The latest post time is: ', data['data'][0]['created_time'])\n",
    "    print('Content:', data['data'][0]['message'])\n",
    "\n",
    "\n",
    "def main():\n",
    "    get_my_friends()\n",
    "    get_page_post(1707015819625206)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imdb電影資訊查詢\n",
    "imdb是很熱門的電影資訊網站, 不過其本身是沒有對外開放API的, 所以這邊會透過一個叫做OMDb API的第三方服務去取得imdb的電影資訊, 要使用OMDb API的服務, 必須要有API key, 這部分請自行付費取得(API_KEY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Please pay for the key yourself.\n",
    "API_KEY = ''\n",
    "OMDB_URL = 'http://www.omdbapi.com/?apikey=' + API_KEY\n",
    "\n",
    "\n",
    "def get_movie_date(url):\n",
    "    data = json.loads(requests.get(url).text)\n",
    "    if data['Response'] == 'True':\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def search_ids_by_keyword(keywords):\n",
    "    movie_ids = list()\n",
    "    # e.g., \"Iron Man\" -> Iron+Man\n",
    "    query = '+'.join(keywords.split())\n",
    "    url = OMDB_URL + '&s=' + query\n",
    "    data = get_movie_date(url)\n",
    "\n",
    "    if data:\n",
    "        for item in data['Search']:\n",
    "            movie_ids.append(item['imdbID'])\n",
    "        total = int(data['totalResults'])\n",
    "        num_pages = math.floor(total/10) + 1\n",
    "\n",
    "        for i in range(2, num_pages+1):\n",
    "            url = OMDB_URL + '&s=' + query + '&page=' + str(i)\n",
    "            data = get_movie_date(url)\n",
    "            if data:\n",
    "                for item in data['Search']:\n",
    "                    movie_ids.append(item['imdbID'])\n",
    "    return movie_ids\n",
    "\n",
    "\n",
    "def search_by_id(movie_id):\n",
    "    url = OMDB_URL + '&i=' + movie_id\n",
    "    data = get_movie_date(url)\n",
    "    return data if data else None\n",
    "\n",
    "\n",
    "def main():\n",
    "    keyword = 'iron man'\n",
    "    m_ids = search_ids_by_keyword(keyword)\n",
    "    print('There are %s movies contain the keyword %s.' % (len(m_ids), keyword))\n",
    "    print('Retrieving movie data...')\n",
    "    movies = list()\n",
    "    for m_id in m_ids:\n",
    "        movies.append(search_by_id(m_id))\n",
    "    print('Top 5 movie results:')\n",
    "    for movie in movies[:5]:\n",
    "        print(movie)\n",
    "    years = [movie['Year'] for movie in movies]\n",
    "    year_dist = Counter(years)\n",
    "    print('Publish year distribution: ', year_dist)\n",
    "    ratings = [float(movie['imdbRating']) for movie in movies if movie['imdbRating'] != 'N/A']\n",
    "    print('Average rating: %.2f' % (sum(ratings)/len(ratings)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Finance API\n",
    "3.4小節的Google Finance個股資訊是直接爬網頁來的, 這邊要示範怎麼透過Google Finance API達到類似的效果."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "GOOGLE_FINANCE_API_URL = 'http://finance.google.com/finance/info?client=ig&q='\n",
    "GOOGLE_FINANCE_HISTORY_API_URL = 'http://www.google.com/finance/getprices?q='\n",
    "\n",
    "\n",
    "def get_stock(query):\n",
    "    # You can query for multiple stocks by splitting with \",\"\n",
    "    resp = requests.get(GOOGLE_FINANCE_API_URL + query)\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url or query param: ' + resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        # Need to remove the redundant chars '//' at the head of response\n",
    "        return json.loads(resp.text.replace('//', ''))\n",
    "\n",
    "\n",
    "def get_stock_history(stock_id, stock_mkt):\n",
    "    resp = requests.get(GOOGLE_FINANCE_HISTORY_API_URL + stock_id + '&x=' + stock_mkt + '&i=86400&p=1M')\n",
    "    ''' e.g.,\n",
    "    EXCHANGE%3DTPE\n",
    "    MARKET_OPEN_MINUTE=540\n",
    "    MARKET_CLOSE_MINUTE=810\n",
    "    INTERVAL=86400\n",
    "    COLUMNS=DATE,CLOSE,HIGH,LOW,OPEN,VOLUME\n",
    "    DATA=\n",
    "    TIMEZONE_OFFSET=480\n",
    "    a1488346200,186,188.5,186,188.5,46176000\n",
    "    1,186,188.5,185,188,39914000\n",
    "    2,184,185,184,184.5,28085000\n",
    "    5,183.5,184.5,183.5,184,12527000\n",
    "    ...\n",
    "    '''\n",
    "    index = -1\n",
    "    records = resp.text.split('\\n')\n",
    "    for record in records:\n",
    "        # 'a' means the start point of stock information\n",
    "        if record.startswith('a'):\n",
    "            index = records.index(record)\n",
    "            break\n",
    "    if index > 0:\n",
    "        records = records[index:]\n",
    "        # To transform the unix time to human readable time at the first line of stock info\n",
    "        unix_time = int(records[0].split(',')[0][1:])\n",
    "        init_time = datetime.fromtimestamp(unix_time)\n",
    "\n",
    "        # To handle to first row\n",
    "        first_row = records[0].split(',')\n",
    "        first_row[0] = init_time\n",
    "\n",
    "        history = list()\n",
    "        history.append(first_row)\n",
    "\n",
    "        # To handle the rest of stock records\n",
    "        for record in records[1:]:\n",
    "            if record:\n",
    "                data = record.split(',')\n",
    "                delta = int(data[0])\n",
    "                data[0] = init_time + timedelta(days=delta)\n",
    "                history.append(data)\n",
    "        return history\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    query = 'TPE:2330'\n",
    "    print('Real time stock price for ' + query)\n",
    "    stocks = get_stock(query)\n",
    "    print(stocks[0])\n",
    "    print('\\n')\n",
    "    stock_id = '2330'\n",
    "    stock_mkt = 'TPE'\n",
    "    print('Stock price history for ' + stock_mkt + \":\" + stock_id)\n",
    "    print('(Date, Close, High, Low, Open, Volume)')\n",
    "    history = get_stock_history(stock_id, stock_mkt)\n",
    "    for hist in history:\n",
    "        print(hist[0].strftime(\"%Y/%m/%d\"), hist[1:])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 台灣證券交易所API\n",
    "這個API長得大概像這樣:<br>\n",
    "http://www.twse.com.tw/exchangeReport/STOCK_DAY?response=json&date=20160501&stockNo=2330 <br>\n",
    "比較重要的地方是date這個參數, 基本上你給的值一定要是yyyyMMdd的形式, 但是真正作用的只有yyyy與MM, 因為他會把這段request解讀成你想要看stockNo股票在yyyy年MM月的紀錄, 所以dd基本上沒有太大意義, 但卻是不可少的部分."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "TWSE_URL = 'http://www.twse.com.tw/exchangeReport/STOCK_DAY?response=json'\n",
    "\n",
    "\n",
    "def get_web_content(stock_id, current_date):\n",
    "    resp = requests.get(TWSE_URL + '&date=' + current_date + '&stockNo=' + stock_id)\n",
    "    if resp.status_code != 200:\n",
    "        return None\n",
    "    else:\n",
    "        return resp.json()\n",
    "\n",
    "\n",
    "def get_data(stock_id, current_date):\n",
    "    info = list()\n",
    "    resp = get_web_content(stock_id, current_date)\n",
    "    if resp is None:\n",
    "        return None\n",
    "    else:\n",
    "        if resp['data']:\n",
    "            for data in resp['data']:\n",
    "                record = {\n",
    "                    '日期': data[0],\n",
    "                    '開盤價': data[3],\n",
    "                    '收盤價': data[6],\n",
    "                    '成交筆數': data[8]\n",
    "                }\n",
    "                info.append(record)\n",
    "        return info\n",
    "\n",
    "\n",
    "def main():\n",
    "    stock_id = '2330'\n",
    "    current_date = time.strftime('%Y%m%d')\n",
    "    current_year = time.strftime('%Y')\n",
    "    current_month = time.strftime('%m')\n",
    "    print('Processing data for %s %s...' % (current_year, current_month))\n",
    "    get_data(stock_id, current_date)\n",
    "    collected_info = get_data(stock_id, current_date)\n",
    "    for info in collected_info:\n",
    "        print(info)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
