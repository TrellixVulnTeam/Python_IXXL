{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "寫爬蟲之前, 可以先看看你想要爬的目標是否已經有人做出工具了, 若有的話就不用費力寫爬蟲了.\n",
    "當然, 若很不幸都沒人做, 你就自己來吧...\n",
    "一定要自己寫爬蟲的時候, 可以按照下面的順序來考慮開發爬蟲的方向:\n",
    "目標網站/服務是否有提供API? (FB, Twitter, Google, etc...)\n",
    "URL/Link有沒有規則可循? (Code, Date, Num, etc...)\n",
    "Response是可解析的Json\n",
    "網頁太複雜的話可以按\"列印此網頁\"或是看看行動版網頁(m.xxx.xxx.com)\n",
    "總而言之, 馬上就開始爬整張網頁一定是最不得已的選項."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 範例一："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of the first h4:\n",
      "<h4 class=\"card-title\">\n",
      "<a href=\"http://www.pycone.com/blogs#pablo\">Mac使用者</a>\n",
      "</h4>\n",
      "\n",
      "Text content of the first h4:\n",
      "Mac使用者\n",
      "\n",
      "To find all the h4 text content:\n",
      "Mac使用者\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "\n",
      "To find all the h4 text content with class named 'card-title' :\n",
      "Mac使用者\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "\n",
      "To find elements with id attribute: \n",
      "在Mac環境下安裝Python與Sublime Text3 Read More\n",
      "[<a data-foo=\"mac-foo\" href=\"http://www.pycone.com/blogs/mac-python-environment\"> <br/>Read More </a>]\n",
      "\n",
      "To retrieve all the blog post's information:\n",
      "開發環境設定 Mac使用者 在Mac環境下安裝Python與Sublime Text3 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (1) 前言 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (2) 套件安裝與啟動網頁爬蟲 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (3) 解構並擷取網頁資料 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (4) 擷取資料及下載圖片 Read More\n",
      "資料科學 給初學者的 Python 網頁爬蟲與資料分析 (5) 資料分析及展示 Read More\n",
      "\n",
      "To find all blog contents via stripped_strings function:\n",
      "['開發環境設定', 'Mac使用者', '在Mac環境下安裝Python與Sublime Text3', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(1) 前言', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(2) 套件安裝與啟動網頁爬蟲', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(3) 解構並擷取網頁資料', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(4) 擷取資料及下載圖片', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(5) 資料分析及展示', 'Read More']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = 'http://blog.castman.net/web-crawler-tutorial/ch2/blog/blog.html'\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    # The following two lines are the same.\n",
    "    # print(soup.find('h4'))\n",
    "    print('Content of the first h4:')\n",
    "    print(soup.h4)\n",
    "\n",
    "    # To find the first text content of anchor of h4\n",
    "    print('\\nText content of the first h4:')\n",
    "    print(soup.h4.a.text)\n",
    "\n",
    "    print('\\nTo find all the h4 text content:')\n",
    "    h4_tags = soup.find_all('h4')\n",
    "    for h4 in h4_tags:\n",
    "        print(h4.a.text)\n",
    "\n",
    "    print('\\nTo find all the h4 text content with class named \\'card-title\\' :')\n",
    "    # The following three ways are the same.\n",
    "    # h4_tags = soup.find_all('h4', {'class': 'card-title'})\n",
    "    # h4_tags = soup.find_all('h4', 'card-title')\n",
    "    h4_tags = soup.find_all('h4', class_='card-title')\n",
    "    for h4 in h4_tags:\n",
    "        print(h4.a.text)\n",
    "\n",
    "    print('\\nTo find elements with id attribute: ')\n",
    "    print(soup.find(id='mac-p').text.strip())\n",
    "    # If the attribute key contains special character, it will occur SyntaxError:\n",
    "    # print(soup.find(data-foo='mac-p').text.strip())\n",
    "    # To prevent this, you can do as the following line:\n",
    "    print(soup.find_all('', {'data-foo': 'mac-foo'}))\n",
    "\n",
    "    print('\\nTo retrieve all the blog post\\'s information:')\n",
    "    divs = soup.find_all('div', 'content')\n",
    "    for div in divs:\n",
    "        # If we only use print(div.text) to retrieve the content, it's not easy to handle the information,\n",
    "        # to make the retrieved data clearly, you can craw the blog page like this:\n",
    "        print(div.h6.text.strip(), div.h4.a.text.strip(), div.p.text.strip())\n",
    "\n",
    "    # There is also another good way the retrieve the blog info, by stripped_strings() function,\n",
    "    # it will return all the text content that are under the parent tag, even wrap by other sub tags.\n",
    "    # However, the return object of stripped_strings is an iterator object, so it's not human-readable.\n",
    "    # To solve this, take a look at following code block:\n",
    "    print('\\nTo find all blog contents via stripped_strings function:')\n",
    "    for div in divs:\n",
    "        # If you feel it's hard to understand, google \"[s for s in subsets(S)]\"\n",
    "        print([s for s in div.stripped_strings])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 範例二：\n",
    "跟前一個範例比起來, 在這種類型的網頁中, find()跟find_all()不見得就是最好用的, 在這種走訪網頁結構的過程中, parent, children, next/previous siblings也可以有很好的效果."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total course count: 6\n",
      "\n",
      "1490\n",
      "1890\n",
      "1890\n",
      "1890\n",
      "1890\n",
      "1890\n",
      "Average course price: 1823.3333333333333\n",
      "\n",
      "Average course price: 1823.3333333333333\n",
      "\n",
      "初心者 - Python入門 初學者 1490 http://www.pycone.com img/python-logo.png\n",
      "Python 網頁爬蟲入門實戰 有程式基礎的初學者 1890 http://www.pycone.com img/python-logo.png\n",
      "Python 機器學習入門實戰 (預計) 有程式基礎的初學者 1890 http://www.pycone.com img/python-logo.png\n",
      "Python 資料科學入門實戰 (預計) 有程式基礎的初學者 1890 http://www.pycone.com img/python-logo.png\n",
      "Python 資料視覺化入門實戰 (預計) 有程式基礎的初學者 1890 http://www.pycone.com img/python-logo.png\n",
      "Python 網站架設入門實戰 (預計) 有程式基礎的初學者 1890 None img/python-logo.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Structure of the example html page:\n",
    "#  body\n",
    "#   - div\n",
    "#     - h2\n",
    "#     - p\n",
    "#     - table.table\n",
    "#       - thead\n",
    "#         - tr\n",
    "#           - th\n",
    "#           - th\n",
    "#           - th\n",
    "#           - th\n",
    "#       - tbody\n",
    "#         - tr\n",
    "#           - td\n",
    "#           - td\n",
    "#           - td\n",
    "#           - td\n",
    "#             - a\n",
    "#               - img\n",
    "#         - tr\n",
    "#         - ...\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = 'http://blog.castman.net/web-crawler-tutorial/ch2/table/table.html'\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    count_course_number(soup)\n",
    "    calculate_course_average_price1(soup)\n",
    "    calculate_course_average_price2(soup)\n",
    "    retrieve_all_tr_contents(soup)\n",
    "\n",
    "\n",
    "def count_course_number(soup):\n",
    "    print('Total course count: ' + str(len(soup.find('table', 'table').tbody.find_all('tr'))) + '\\n')\n",
    "\n",
    "\n",
    "def calculate_course_average_price1(soup):\n",
    "    # To calculate the average course price\n",
    "    # Retrieve the record with index:\n",
    "    prices = []\n",
    "    rows = soup.find('table', 'table').tbody.find_all('tr')\n",
    "    for row in rows:\n",
    "        price = row.find_all('td')[2].text\n",
    "        print(price)\n",
    "        prices.append(int(price))\n",
    "    print('Average course price: ' + str(sum(prices) / len(prices)) + '\\n')\n",
    "\n",
    "\n",
    "def calculate_course_average_price2(soup):\n",
    "    # Retrieve the record via siblings:\n",
    "    prices = []\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        price = link.parent.previous_sibling.text\n",
    "        prices.append(int(price))\n",
    "    print('Average course price: ' + str(sum(prices) / len(prices)) + '\\n')\n",
    "\n",
    "\n",
    "def retrieve_all_tr_contents(soup):\n",
    "    # Retrieve all tr record:\n",
    "    rows = soup.find('table', 'table').tbody.find_all('tr')\n",
    "    for row in rows:\n",
    "        # Except all_tds = row.find_all('td'), you can also retrieve all td record with the following line code:\n",
    "        all_tds = [td for td in row.children]\n",
    "        if 'href' in all_tds[3].a.attrs:\n",
    "            href = all_tds[3].a['href']\n",
    "        else:\n",
    "            href = None\n",
    "        print(all_tds[0].text, all_tds[1].text, all_tds[2].text, href, all_tds[3].a.img['src'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 範例三(加入Regular Expression)：\n",
    "有些時候可能要找含有某些特定pattern的內容, 如電話, email, url, 特定的tag(h4)等等..., 這時候如果會用regular expression就可以比較有效率的取出需要的資訊."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些常見的pattern:\n",
    "* URL: http(s)?://[a-zA-Z0-9./_]+\n",
    "* Email: [a-zA-Z0-9._+]+@[a-zA-Z0-9._]+.(com|org|edu|gov|net)\n",
    "* 所有的中文字(不包含標點符號): [\\u4e00-\\u9fa5]+\n",
    "* 線上Unicode查詢: http://unicodelookup.com\n",
    "* 自己google別人寫好的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python教學文章\n",
      "開發環境設定\n",
      "Mac使用者\n",
      "資料科學\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "資料科學\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "資料科學\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "資料科學\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "資料科學\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "\n",
      "Find all .png img source:\n",
      "static/python-for-beginners.png\n",
      "static/python_crawler.png\n",
      "static/python_crawler.png\n",
      "static/python_crawler.png\n",
      "static/python_crawler.png\n",
      "static/python_crawler.png\n",
      "\n",
      "Find all .png img sources that contain \"beginner\" in file name:\n",
      "static/python-for-beginners.png\n",
      "\n",
      "To count the blog number:\n",
      "Blog count: 6\n",
      "\n",
      "To find how many image sources contains the word \"crawler\"\n",
      "static/python_crawler.png\n",
      "static/python_crawler.png\n",
      "static/python_crawler.png\n",
      "static/python_crawler.png\n",
      "static/python_crawler.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = 'http://blog.castman.net/web-crawler-tutorial/ch2/blog/blog.html'\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    find_text_content_by_reg(soup, 'h[1-6]')\n",
    "\n",
    "    # [a-zA-Z0-9]+ -> means that we hope the result string is composed by character a~z, A~Z and 0~9,\n",
    "    # and the string length should ≥ 1 (which represented by \"+\").\n",
    "\n",
    "    # http(s)?://[a-zA-Z0-9\\./_]+ -> means hyper link.\n",
    "\n",
    "    # [\\u4e00-\\u9fa5]+ -> means all the chinese words in unicode format.\n",
    "\n",
    "    print('\\nFind all .png img source:')\n",
    "    # To find png type image source by reg.\n",
    "    # $ means the tail, the end of the string.\n",
    "    # \\. means \".\", the \\ is for escaping the special characters.\n",
    "    png_source_pattern = '\\.png$'\n",
    "    find_img_source_by_reg(soup, png_source_pattern)\n",
    "\n",
    "    # To find png type image source which contains \"beginner\" in source name by reg.\n",
    "    # In the pattern, the \".\" after beginner means any words,\n",
    "    # the * means the length is 0 or 1.\n",
    "    print('\\nFind all .png img sources that contain \\\"beginner\\\" in file name:')\n",
    "    find_img_source_by_reg(soup, 'beginner.*'+png_source_pattern)\n",
    "\n",
    "    print('\\nTo count the blog number:')\n",
    "    blog_class_pattern = 'card\\-blog$'\n",
    "    count_blog_number(soup, blog_class_pattern)\n",
    "\n",
    "    print('\\nTo find how many image sources contains the word \\\"crawler\\\"')\n",
    "    target_pattern = 'crawler.*'\n",
    "    find_img_source_by_reg(soup, target_pattern)\n",
    "\n",
    "\n",
    "# re.compile API DOC: https://docs.python.org/3/library/re.html#re.compile\n",
    "def find_text_content_by_reg(soup, reg_pattern):\n",
    "    for element in soup.find_all(re.compile(reg_pattern)):\n",
    "        print(element.text.strip())\n",
    "\n",
    "\n",
    "def find_img_source_by_reg(soup, source_type):\n",
    "    for img in soup.find_all('img', {'src': re.compile(source_type)}):\n",
    "        print(img['src'])\n",
    "\n",
    "\n",
    "def count_blog_number(soup, blog_pattern):\n",
    "    count = len(soup.find_all('div', {'class': re.compile(blog_pattern)}))\n",
    "    print('Blog count: ' + str(count))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
