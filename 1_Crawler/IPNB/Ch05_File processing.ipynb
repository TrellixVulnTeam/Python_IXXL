{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.itread01.com/content/1544469144.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 開啟檔案 open()<br>\n",
    "open('檔案路徑/檔名', mode='模式')\n",
    "\n",
    "基本模式(進階可在公式按shift+tab查看)：\n",
    "### 1. \"r\" 唯讀模式:\n",
    "當我們以mode為'r'開啟檔案時，意即我們只能從指定的檔案讀取資料，並不能夠對這個檔案的內容進行更動。<br>\n",
    "與此同時，如果我們所指定的檔案不存在，將會產生 FileNotFoundError 的例外。<br>\n",
    "\n",
    "### 2. \"w\" 寫入模式(覆寫):\n",
    "而如果我們以'w'為開啟檔案的模式時，意即要來進行檔案的寫入，<br>\n",
    "會在開啟的位置直接覆蓋掉原本的檔案。如果我們所指定的檔案路徑/名稱不存在，會新增一個新的檔案。<br>\n",
    "\n",
    "### 3. \"a\" 寫入模式(續寫):\n",
    "但若我們設定開啟檔案的模式為'a'，是指在此模式下開啟檔案要進行寫入時，會從原本的檔案最後繼續進行寫入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.NavigableString'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.NavigableString'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.NavigableString'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.NavigableString'>\n"
     ]
    }
   ],
   "source": [
    "#Ch5_1\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "# 使用childen屬性取得子標籤\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "tag_ul = tag_div[0].ul       # 走訪到之下的<ul>\n",
    "for child in tag_ul.children:\n",
    "    print(type(child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "li\n",
      "li\n",
      "li\n"
     ]
    }
   ],
   "source": [
    "#Ch5_1a\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "# 使用childen屬性取得子標籤\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "tag_ul = tag_div[0].ul       # 走訪到之下的<ul>\n",
    "for child in tag_ul.children:\n",
    "    if not isinstance(child, NavigableString):\n",
    "        print(child.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試資料擷取的HTML網頁\n",
      "utf-8\n",
      "請問你的性別?\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_1\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "# 使用屬性向下走訪\n",
    "print(soup.html.head.title.string)\n",
    "print(soup.html.head.meta[\"charset\"])\n",
    "# 使用div屬性取得第1個<div>標籤\n",
    "print(soup.html.body.div.div.p.a.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "20\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_1a\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "# 使用屬性取得所有子標籤\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "tag_ul = tag_div[0].ul       # 走訪到之下的<ul>\n",
    "for child in tag_ul.contents:\n",
    "    if not isinstance(child, NavigableString):\n",
    "        print(child.span.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "li\n",
      "喜歡-\n",
      "span 40\n",
      "li\n",
      "普通-\n",
      "span 20\n",
      "li\n",
      "不喜歡-\n",
      "span 0\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_1b\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "# 使用屬性取得所有子標籤\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "tag_ul = tag_div[0].ul       # 走訪到之下的<ul>            \n",
    "for child in tag_ul.children:\n",
    "    if not isinstance(child, NavigableString):\n",
    "        print(child.name)\n",
    "        for tag in child:\n",
    "            if not isinstance(tag, NavigableString):\n",
    "                print(tag.name, tag.string)\n",
    "            else:\n",
    "                print(tag.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "li\n",
      "span\n",
      "li\n",
      "span\n",
      "li\n",
      "span\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_1c\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4.element import NavigableString\n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "# 使用屬性取得所有子孫標籤\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "tag_ul = tag_div[0].ul       # 走訪到之下的<ul>            \n",
    "for child in tag_ul.descendants:\n",
    "    if not isinstance(child, NavigableString):\n",
    "        print(child.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "喜歡-\n",
      "40\n",
      "\n",
      "普通-\n",
      "20\n",
      "\n",
      "不喜歡-\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_1d\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "# 使用屬性取得所有子孫的文字內容\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "tag_ul = tag_div[0].ul       # 走訪到之下的<ul>\n",
    "for string in tag_ul.strings:\n",
    "    print(string.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div\n",
      "div\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_2\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "tag_ul = tag_div[0].ul       # 走訪到之下的<ul>\n",
    "# 使用屬性取得父標籤\n",
    "print(tag_ul.parent.name)\n",
    "# 使用函數取得父標籤\n",
    "print(tag_ul.find_parent().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div\n",
      "div\n",
      "body\n",
      "html\n",
      "[document]\n",
      "div\n",
      "div\n",
      "body\n",
      "html\n",
      "[document]\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_2a\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "tag_ul = tag_div[0].ul       # 走訪到之下的<ul>\n",
    "# 使用屬性取得所有祖先標籤\n",
    "for tag in tag_ul.parents:\n",
    "    print(tag.name)\n",
    "# 使用函數取得所有祖先標籤\n",
    "for tag in tag_ul.find_parents():\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li class=\"response\">喜歡-<span>40</span></li>\n",
      "<li class=\"response selected\">普通-<span>20</span></li>\n",
      "<li class=\"response\">不喜歡-<span>0</span></li>\n",
      "---------------------------------------\n",
      "li 20\n",
      "li 0\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_3\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "first_li = tag_div[0].ul.li  # 第1個<li>\n",
    "print(first_li)\n",
    "# 使用next_sibling屬性取得下一個兄弟標籤\n",
    "second_li = first_li.next_sibling.next_sibling\n",
    "print(second_li)\n",
    "# 呼叫next_sibling()函數取得下一個兄弟標籤\n",
    "third_li = second_li.find_next_sibling()\n",
    "print(third_li)\n",
    "print(\"---------------------------------------\")\n",
    "# 呼叫next_siblings()函數取得所有兄弟標籤\n",
    "for tag in first_li.find_next_siblings():\n",
    "    print(tag.name, tag.span.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li class=\"response\">不喜歡-<span>0</span></li>\n",
      "<li class=\"response selected\">普通-<span>20</span></li>\n",
      "<li class=\"response\">喜歡-<span>40</span></li>\n",
      "---------------------------------------\n",
      "li 20\n",
      "li 40\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_3a\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "tag_div = soup.select(\"#q2\") # 找到第2題\n",
    "tag_li = tag_div[0].ul.li  # 第1個<li>\n",
    "third_li = tag_li.find_next_sibling().find_next_sibling()\n",
    "print(third_li)\n",
    "# 使用previous_sibling屬性取得前一個兄弟標籤\n",
    "second_li = third_li.previous_sibling.previous_sibling\n",
    "print(second_li)\n",
    "# 呼叫previous_sibling()函數取得前一個兄弟標籤\n",
    "first_li = second_li.find_previous_sibling()\n",
    "print(first_li)\n",
    "print(\"---------------------------------------\")\n",
    "# 呼叫previous_siblings()函數取得所有兄弟標籤\n",
    "for tag in third_li.find_previous_siblings():\n",
    "    print(tag.name, tag.span.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'> html\n",
      "<class 'bs4.element.Tag'> head\n",
      "<class 'bs4.element.Tag'> title\n",
      "<class 'bs4.element.Tag'> meta\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_4\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")\n",
    "tag_html = soup.html # 找到第<html>標籤\n",
    "print(type(tag_html), tag_html.name)\n",
    "tag_next = tag_html.next_element.next_element\n",
    "print(type(tag_next), tag_next.name)\n",
    "tag_title = soup.title # 找到第<title>標籤\n",
    "print(type(tag_title), tag_title.name)\n",
    "tag_previous = tag_title.previous_element.previous_element\n",
    "print(type(tag_previous), tag_previous.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch5_2_4a\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4.element import NavigableString\n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")    \n",
    "tag_div = soup.find(id = \"emails\")\n",
    "for element in tag_div.next_elements:\n",
    "    if not isinstance(element, NavigableString):\n",
    "        print(element.name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div\n",
      "body\n",
      "title\n",
      "meta\n",
      "head\n",
      "html\n"
     ]
    }
   ],
   "source": [
    "#Ch5_2_4b\n",
    "from bs4 import BeautifulSoup  \n",
    "from bs4.element import NavigableString\n",
    "\n",
    "with open(\"Example.html\", \"r\", encoding=\"utf8\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\")    \n",
    "tag_div = soup.find(id=\"q1\")\n",
    "for element in tag_div.previous_elements:\n",
    "    if not isinstance(element, NavigableString):\n",
    "        print(element.name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"question\" id=\"name\">Joe</p>\n",
      "<p id=\"name\">Joe</p>\n"
     ]
    }
   ],
   "source": [
    "#Ch5_3\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "soup = BeautifulSoup(\"<b class='score'>Joe</b>\", \"lxml\")    \n",
    "tag = soup.b\n",
    "tag.name = \"p\"\n",
    "tag[\"class\"] = \"question\"\n",
    "tag[\"id\"] = \"name\"\n",
    "print(tag)\n",
    "del tag[\"class\"]\n",
    "print(tag)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b class=\"score\">Mary</b>\n"
     ]
    }
   ],
   "source": [
    "#Ch5_3a\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "soup = BeautifulSoup(\"<b class='score'>Joe</b>\", \"lxml\")    \n",
    "tag = soup.b\n",
    "tag.string = \"Mary\"\n",
    "print(tag)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>Joe</b>\n",
      "<b>Joe Chen</b>\n",
      "<b>Joe Chen<a href=\"http://www.example.com\"></a></b>\n"
     ]
    }
   ],
   "source": [
    "#Ch5_3b\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString \n",
    "\n",
    "soup = BeautifulSoup(\"<b></b>\", \"lxml\")    \n",
    "tag = soup.b\n",
    "tag.append(\"Joe\")\n",
    "print(tag)\n",
    "new_str = NavigableString(\" Chen\")\n",
    "tag.append(new_str)\n",
    "print(tag)\n",
    "new_tag = soup.new_tag(\"a\", href=\"http://www.example.com\")\n",
    "tag.append(new_tag)\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><i>Two</i><b>One</b></p>\n",
      "<p><i>Two</i><b>One</b>Three</p>\n",
      "<p><i>Two</i><b></b>Three</p>\n"
     ]
    }
   ],
   "source": [
    "#Ch5_3c\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "soup = BeautifulSoup(\"<p><b>One</b></p>\", \"lxml\")  \n",
    "tag = soup.b  \n",
    "new_tag = soup.new_tag(\"i\")\n",
    "new_tag.string = \"Two\"\n",
    "tag.insert_before(new_tag)\n",
    "print(soup.p)\n",
    "new_string = soup.new_string(\"Three\")\n",
    "tag.insert_after(new_string)\n",
    "print(soup.p)\n",
    "tag.clear()\n",
    "print(soup.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><i>Two</i></p>\n"
     ]
    }
   ],
   "source": [
    "#Ch5_3d\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "soup = BeautifulSoup(\"<p><b>One</b></p>\", \"lxml\")  \n",
    "tag = soup.b  \n",
    "new_tag = soup.new_tag(\"i\")\n",
    "new_tag.string = \"Two\"\n",
    "tag.replace_with(new_tag)\n",
    "print(soup.p)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data1,Data2,Data3\n",
      "10,33,45\n",
      "5,25,56\n"
     ]
    }
   ],
   "source": [
    "#Ch5_4_1\n",
    "import csv\n",
    "\n",
    "csvfile = \"Example.csv\"\n",
    "with open(csvfile, 'r') as fp:\n",
    "    reader = csv.reader(fp)\n",
    "    for row in reader:\n",
    "        print(','.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch5_4_1a\n",
    "import csv\n",
    "\n",
    "csvfile = \"Example2.csv\"\n",
    "list1 = [[10,33,45], [5, 25, 56]]\n",
    "with open(csvfile, 'w+', newline='') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([\"Data1\",\"Data2\",\"Data3\"])\n",
    "    for row in list1:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch5_4_1b\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = \"https://www.w3schools.com/html/html_media.asp\"\n",
    "csvfile = \"VideoFormat.csv\"\n",
    "r = requests.get(url)\n",
    "r.encoding = \"utf8\"\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "tag_table = soup.find(class_=\"w3-table-all\")  # 找到<table>\n",
    "rows = tag_table.findAll(\"tr\")   # 找出所有<tr>\n",
    "# 開啟CSV檔案寫入截取的資料\n",
    "with open(csvfile, 'w+', newline='', encoding=\"utf-8\") as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    for row in rows:\n",
    "        rowList = []\n",
    "        for cell in row.findAll([\"td\", \"th\"]):\n",
    "            rowList.append(cell.get_text().replace(\"\\n\", \"\").replace(\"\\r\", \"\"))\n",
    "        writer.writerow(rowList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Joe Chen\", \"score\": 95, \"tel\": \"0933123456\"}\n",
      "{'name': 'Joe Chen', 'score': 95, 'tel': '0933123456'}\n"
     ]
    }
   ],
   "source": [
    "#Ch5_4_2\n",
    "import json\n",
    "\n",
    "data = {\n",
    "   \"name\": \"Joe Chen\", \n",
    "   \"score\": 95, \n",
    "   \"tel\": \"0933123456\"         \n",
    "}\n",
    "\n",
    "json_str = json.dumps(data)\n",
    "print(json_str)\n",
    "data2 = json.loads(json_str)\n",
    "print(data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch5_4_2a\n",
    "import json\n",
    "\n",
    "data = {\n",
    "   \"name\": \"Joe Chen\", \n",
    "   \"score\": 95, \n",
    "   \"tel\": \"0933123456\"        \n",
    "}\n",
    "\n",
    "jsonfile = \"Example.json\"\n",
    "with open(jsonfile, 'w') as fp:\n",
    "    json.dump(data, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Joe Chen\", \"score\": 95, \"tel\": \"0933123456\"}\n"
     ]
    }
   ],
   "source": [
    "#Ch5_4_2b\n",
    "import json\n",
    "\n",
    "jsonfile = \"Example.json\"\n",
    "with open(jsonfile, 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "json_str = json.dumps(data)    \n",
    "print(json_str)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch5_4_2c\n",
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"https://www.googleapis.com/books/v1/volumes?maxResults=5&q=Python&projection=lite\"\n",
    "jsonfile = \"Books.json\"\n",
    "r = requests.get(url)\n",
    "r.encoding = \"utf8\"\n",
    "json_data = json.loads(r.text)\n",
    "with open(jsonfile, 'w') as fp:\n",
    "    json.dump(json_data, fp)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "錯誤! HTTP請求失敗...\n"
     ]
    }
   ],
   "source": [
    "#Ch5_5\n",
    "import requests\n",
    "\n",
    "url = \"http://hueyanchen.myweb.hinet.net/fchart05.png\"\n",
    "path = \"fchart05.png\"\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(path, 'wb') as fp:\n",
    "        for chunk in response:\n",
    "            fp.write(chunk)\n",
    "    print(\"圖檔已經下載\")        \n",
    "else:\n",
    "    print(\"錯誤! HTTP請求失敗...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-af4455cab88f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"http://hueyanchen.myweb.hinet.net/fchart05.png\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fchart06.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "#Ch5_5a\n",
    "import urllib.request\n",
    "\n",
    "url = \"http://hueyanchen.myweb.hinet.net/fchart05.png\"\n",
    "response = urllib.request.urlopen(url)\n",
    "fp = open(\"fchart06.png\", \"wb\")\n",
    "size = 0\n",
    "while True:\n",
    "    info = response.read(10000)\n",
    "    if len(info) < 1:\n",
    "        break\n",
    "    size = size + len(info)\n",
    "    fp.write(info)    \n",
    "print(size, \"個字元下載...\")\n",
    "fp.close()\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\n",
      "圖檔logo.png已經下載\n"
     ]
    }
   ],
   "source": [
    "#Ch5_5b\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://www.google.com.tw\"\n",
    "path = \"logo.png\"\n",
    "r = requests.get(url)\n",
    "r.encoding = \"utf8\"\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "tag_a = soup.find(id=\"hplogo\")\n",
    "# 取出Logo圖片的正規運算式\n",
    "match = re.search(r\"(/[^/#?]+)+\\.(?:jpg|gif|png)\", str(tag_a))\n",
    "print(match.group())\n",
    "url = url + str(match.group())\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(path, 'wb') as fp:\n",
    "        for chunk in response:\n",
    "            fp.write(chunk)\n",
    "    print(\"圖檔logo.png已經下載\")        \n",
    "else:\n",
    "    print(\"錯誤! HTTP請求失敗...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img alt=\"Google\" height=\"92\" id=\"hplogo\" src=\"/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\" style=\"padding:28px 0 14px\" width=\"272\"/>\n"
     ]
    }
   ],
   "source": [
    "#Ch5_5c\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://www.google.com.tw\"\n",
    "r = requests.get(url)\n",
    "r.encoding = \"big5\"\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "tag_a = soup.find(id=\"hplogo\")\n",
    "print(tag_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 關閉檔案 close()\n",
    "當我們在程式當中開啟了檔案以後，如果要停止對於這個檔案的更動或寫入，可以將檔案關閉。此時所使用的函式如下：<br>\n",
    "f.close()<br>\n",
    "當我們要開啟其他檔案時，當然可以直接再開啟然後取代掉目前的 file object，但是記得在使用時關閉檔案依然是一個好習慣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取檔案 read()\n",
    "file.read([size])<br>\n",
    "當我們設置了 size 的值，電腦就會自動讀到指定的字節數量，若沒有設置就會將整個檔案都讀取進來。<br>\n",
    "ex:file.read(6)就只讀取前六個字節。<br>\n",
    "    \n",
    "file.readline()<br>\n",
    "讀取檔案中的整行資料，但是一次只讀取一行，包含 \\n 字元，如果檔案當中包含了 N 行的資料，我們就必須呼叫 f.readline() N 次。<br>\n",
    "\n",
    "file.readlines()<br>\n",
    "將檔案當中的所有資料都逐行讀取進來，然後會將其回傳成為一個 list。<br>\n",
    "由此可見， f.readlines() 會將檔案當中每一行當作是一個字串，然後存進串列當中回傳。因此 for loop經常與 f.readlines() 搭配使用，方法如下：<br>\n",
    "for line in f.readlines():<br>\n",
    "    print(line)<br>\n",
    "\n",
    "除此之外， Python 還給了我們一個幾乎一模一樣的用法，在不使用 f.readlines() 的情況下，<br>\n",
    "我們可以直接利用 for loop 對於我們的 file object f 來 iterate，方法如下：<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "    \n",
    "我們會得到相同的執行結果<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寫入檔案 write()\n",
    "\n",
    "file.write('string')\n",
    "參數的資料型態是字串\n",
    "\n",
    "file.writelines(seq)\n",
    "seq參數必須是一個序列，也就是 list 或是 tuple 這類的資料型態。<br>\n",
    "\n",
    "print()\n",
    "當然，貼心的 Python 也提供了我們利用熟悉的 print() 就可以完成輸出到檔案的方法。<br>\n",
    "當我們今天想要將任何東西輸出時，都會直接使用 print() ，然後頂多更改參數 sep 或是 end 。<br>\n",
    "然而，除了這兩個參數以外，還有第三個參數 file 。方法如下：<br>\n",
    "f = open('movies.txt','w')\n",
    "print(list, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在檔案中移動位置\n",
    "\n",
    "接著，我們要來介紹如何更進一步的進行檔案的處理。當我們在進行檔案的讀取時，有時可能會想要將檔案從頭到尾讀取第二遍。<br>\n",
    "那麼，當我們想要將上面的檔案內容印出兩次時，可能會寫出如下的程式碼進行以下的操作：<br>\n",
    "f = open('sample.txt')<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "f.close()<br>\n",
    "在我們的想像當中，上面的程式碼在第一個 for loop 跑完以後，下一個 for loop 應該要將檔案的內容在輸出一次，<br>\n",
    "也就是說，我們心裡想的輸出結果應該要是這樣子的：<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "然而，執行結果卻如下：<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "很顯然的，第二次的 for loop 完全沒有印出東西來。<br>\n",
    "原因是這樣子的，當我們在讀取或是寫入檔案時，可以想像成電腦裡面有一個指標，<br>\n",
    "類似於我們在使用 word 或是任何的文字編輯器時的指標，我們會依照他的位置，<br>\n",
    "知道我們接下來如果要打字會出現在什麼地方。<br>\n",
    "同樣的，當 file object 在處理檔案時也是一樣的概念。在上面的範例當中，<br>\n",
    "第一個 for loop 結束以後，這個指針隨著我們的讀取已經到了文件的最後面。<br>\n",
    "因此，接下來想要再使用 for loop 來讀取的話，如果我們沒有將指針移到檔案最前面，就無法得到任何東西。<br>\n",
    "因此，以下要介紹用來移動這個指針或是獲取現在指針位置的方法。<br>\n",
    "\n",
    "# file.seek()\n",
    "第一個是 f.seek() ，顧名思義，就是在檔案當中尋找。不過這裡說的尋找指的並不是尋找某個字或是某個資料，而是尋找某個位置，用法如下。<br>\n",
    "f = open('sample.txt')<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "f.seek(0)<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "f.close()<br>\n",
    "結果就會是兩遍：<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "\n",
    "# file.tell()\n",
    "他和 f.seek() 接近於相對的概念，所謂的 f.tell() ，顧名思義就是會回傳給你現在指標所在的位置。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
